# kafka地址与端口号 e: e.x:172.18.18.100:9092,172.18.18.101:9092,172.18.18.102:9092
kafka.bootstrap.servers=172.18.18.100:9092,172.18.18.101:9092,172.18.18.102:9092
kafka.request.required.acks=-1
kafka.maximum.time=1000
kafka.retries=0
kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.topic=feature111
kafka.group.id=group222

#zookeeper连接地址与端口 e: e.x:172.18.18.100:2181,172.18.18.101:2181,172.18.18.102:2181
zookeeper.address=172.18.18.100:2181,172.18.18.101:2181,172.18.18.102:2181

#worker参数配置
worker.buffer.size.max=500
#内存中缓存数据的最大值
worker.cach.size.max=5000000
#内存数据的检查时间间隔
worker.memory.check.time=1800000
#内存中记录的过期时间
work.record.time.out=90
#检查任务列表的时间间隔
work.check_task.time=1000
#文件的过期时间
worker.file.time.out=
#文件检查时间间隔
worker.file.check.time=1800000
#写HBase任务的时间间隔
worker.hbase.write.time=2000
#数据持久化方案（一个文件保存几天的数据）
worker.file.save.program=1
#文件保存路径
worker.file.path=test
#文件保存大小
worker.file.size=
#项目启动时，加载多少天的数据到内存中
worker.load.data.days=90
#文件流过期时间
worker.stream.time.out=2
#数据持久化的文件系统 0 本地  1 HDFS
worker.file.save.system=0
#worker绑定本地端口
worker.port=8888

worker.readfiles_per_thread

worker.address=localhost
worker.rpc.port=8889

worker.executors.to.compare=10
worker.executors.to.loadfile=15

worker.id=1

#目录根路径
root.path=D:/test

#是否删除过期文件
delete.open=1
